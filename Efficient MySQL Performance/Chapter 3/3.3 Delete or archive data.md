
### 1. Tools

You will most likely need to **write your own tools** for deleting or archiving data. While the process itself might be "trivial compared to your application," the **critically important part is throttling the loop that executes SQL statements**. Directly executing large `DELETE` statements with very high `LIMIT` values (e.g., `LIMIT 1000000`) in a fast loop is highly likely to cause an application outage.

### 2. Batch Size

Batch size is the **key to a safe and effective data archiving tool**. It refers to the number of rows deleted per `DELETE` statement, controlled by a `LIMIT` clause, and, if necessary, throttled by a simple delay.

- **Manual Deletion Shortcut:** For small-scale operations, it's generally **safe to manually delete 1,000 rows or less** in a single `DELETE` statement. This applies if the rows are small (no `BLOB`, `TEXT`, or `JSON` columns) and MySQL is not heavily loaded. This works because humans are typically too slow to manually execute these statements fast enough to overload MySQL. This shortcut should be used judiciously, and manual deletes should be reviewed by another engineer.
- **Calibrating Execution Time:** The safe rate for deleting rows is determined by a **calibrated batch size** that MySQL and the application can sustain without impacting query response time or replication lag. A good starting point for calibration is aiming for **500 milliseconds for each `DELETE` statement**. This target is crucial for two main reasons:
    - **Replication Lag:** Execution time on the source MySQL instance directly creates replication lag on replica instances. A 500 ms `DELETE` on the source will create 500 ms of replication lag on the replica. This lag must be minimized because **replication lag is data loss**.
    - **Throttling:** While a calibrated batch size can limit query execution time (e.g., a 500 ms query can run at 2 QPS in series), bulk writes can still disrupt other queries and the application without explicit throttling. **Throttling is paramount** in bulk operations; you should always start with a delay between `DELETE` statements and monitor replication lag.
- **Iterative Calibration Process:**
    - Begin with a batch size of 1,000 (`LIMIT 1000`) and a **200 ms delay** between `DELETE` statements.
    - Run this for at least 10 minutes while monitoring **replication lag and MySQL stability**.
    - Inspect the **maximum execution time** of the `DELETE` statement (via query reporting or direct measurement).
    - If the maximum execution time is well below the 500 ms target, **double the batch size** and repeat the 10-minute run. Continue iteratively increasing the batch size until the maximum execution time is consistently on target, preferably slightly below.
    - Once the batch size is calibrated, **slowly reduce the delay** between `DELETE` statements in 10-minute reruns. Stop if replication lag or MySQL destabilization occurs, then revert to the last stable delay.
- **Rate Calculation:** With the calibrated batch size and set throttle, you can calculate the deletion rate: `batch size * DELETE QPS`. This rate may vary, and for ambitious operations, a higher rate can be attempted during quiet periods (e.g., middle of the night), but remember to reset before peak load.

### 3. Row Lock Contention

For **write-heavy workloads**, bulk operations, including `DELETE`s, can lead to elevated **row lock contention**, where queries wait to acquire row locks on the same or nearby rows. This is particularly problematic if deleted rows are interspersed with rows being kept. Even if the `DELETE` statement executes within the calibrated time, a large batch size can still cause contention if locks overlap with active application updates.

The solution to high row lock contention is to **reduce the batch size** further, calibrating for a much smaller execution time (e.g., 100 ms). In extreme cases, you might also need to increase the delay, resulting in a small batch size with a long delay. This reduces contention but slows down the data archiving process.

### 4. Space and Time

A crucial point often misunderstood is that **deleting data does NOT immediately free disk space**. ==Row deletes are logical, not physical, meaning they create "free pages" within the InnoDB tablespace rather than reclaiming disk space on the operating system level.==

- **Reclaiming Disk Space:** To reclaim disk space, you must **rebuild the table**. This can be done online (in production without impacting the application) using tools such as:
    - `pt-online-schema-change`
    - `gh-ost`
    - ==`ALTER TABLE...ENGINE=INNODB` (which defaults to an in-place alter, avoiding a full table copy)==
- **Time Commitment:** Deleting large amounts of data, and subsequently reclaiming disk space by rebuilding tables, **takes time**. These are bulk operations that must be executed with significant restraint to avoid impacting the application, meaning they will take "a lot longer than you think". A well-calibrated, sustainable bulk operation can run for days or even weeks.

### 5. The Binary Log Paradox

**Deleting data paradoxically creates more data** because data changes are written to the binary logs. Binary logging is essential for replication, and production systems typically run with replicas.

- **`binlog_row_image`:** If the table contains large `BLOB`, `TEXT`, or `JSON` columns, the binary log size can increase dramatically because the MySQL system variable `binlog_row_image` defaults to `full`, meaning it writes the value of every column.
    - Setting `binlog_row_image` to `minimal` (writes only changed columns and those needed to identify the row) or `noblob` (writes all columns except `BLOB` and `TEXT` unless required) is **safe and recommended** if external services do not rely on full row images. This can significantly reduce binary log size.
- **Disk Usage During Operations:** During deletion and table rebuilds (especially with tools like `pt-online-schema-change` or `gh-ost` that copy tables), **disk usage will temporarily increase** due to binary logging and the fact that space isn't immediately freed. Therefore, you must paradoxically **ensure that the server has enough free disk space** to perform these operations.
