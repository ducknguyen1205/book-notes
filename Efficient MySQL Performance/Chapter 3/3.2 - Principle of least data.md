The 'Principle of Least Data' is a core concept introduced in Chapter 3 of "Efficient MySQL Performance," which emphasizes that **reducing data size and access fundamentally improves MySQL performance**. This principle is presented as an **indirect query optimization technique**, used when direct query optimizations (like indexing) are no longer sufficient. The author argues that less data is better for everything, including **performance, management, and cost**.

The principle of least data is defined as: "**store and access only needed data**". This seemingly obvious theory is often far from the norm in practice. The chapter examines this principle through two primary lenses: **data access** and **data storage**.

### Data Access

**"Do not access more data than needed"**. Data access refers to all the work MySQL performs to execute a query, including finding, processing, and returning rows for both reads (SELECT) and writes. Efficient data access is particularly crucial for writes due to the difficulty in scaling them.

The sources provide an "Efficient data access checklist" to verify efficiency:

- **Return only needed columns**: Queries should **never use `SELECT *`**, especially if tables contain `BLOB`, `TEXT`, or `JSON` columns. This is a decades-old best practice in the database industry.
- **Reduce query complexity**: Simpler queries tend to access less data because they involve fewer tables, conditions, and SQL clauses, resulting in less work for MySQL. However, any simplification must be confirmed with an `EXPLAIN` plan to ensure it doesn't lead to a worse execution plan or change the result set.
- **Limit row access**: This is critical for both reads and writes.
    - A `LIMIT` clause itself does not inherently limit row access; it applies _after_ rows are matched.
    - The **`ORDER BY...LIMIT` optimization** is an exception, where MySQL can stop reading rows once the `LIMIT` number of matching rows are found, _if_ it can access rows in index order. This is proven by `Rows_examined` in query metrics being equal to `Rows_sent` when the optimization is effective.
    - Applications should also **limit the input used in a query**, as a large number of input values can cause MySQL to switch query execution plans, leading to slow performance (e.g., thousands of values in an `IN` list).
    - For writes, limiting row access is critical because InnoDB locks every accessed row before updating matching ones, potentially locking more rows than expected.
    - For table joins, limiting row access is paramount. Even with an index, a join can struggle if the index doesn't match a single row, as a nonunique index lookup can still access many duplicate rows.
    - You should know your access patterns, use `EXPLAIN` to see `rows` estimates, and monitor `rows examined` to avoid surprises.
    - **An example of incredibly efficient data access** mentioned earlier in the book (Chapter 1) is a primary key lookup returning only two columns from a single row, which allowed for an "incredible QPS and query load" (e.g., query load 5,962). This level of efficiency makes MySQL function almost like an in-memory cache.
- **Limit the result set**: This involves preventing the application from retrieving more rows than it actually uses.
    - **Application using some, but not all, rows**: This can be unintentional (indicating the `WHERE` clause needs more selective conditions) or intentional (shifting row filtering to the application to avoid complex queries).
    - **Application using an ordered subset of rows**: If a query returns many rows but the application only uses a small ordered subset (e.g., for pagination), adding a `LIMIT` clause or `LIMIT...OFFSET` with the `ORDER BY...LIMIT` optimization can prevent wasted work.
    - **Application only aggregates the result set**: Instead of having the application perform aggregations, **SQL aggregate functions** (like `SUM`, `COUNT`, `DISTINCT`) should be used to limit the result set returned by MySQL. `COUNT(*)` counts matching rows, while `COUNT(column)` counts non-NULL values. `DISTINCT` can also be used to extract unique values, effectively limiting the result set.

### Data Storage

**"Do not store more data than needed"**. The author states that data, while valuable, is "dead weight to MySQL". Proactively addressing data size before it becomes a problem is key.

The "Efficient data storage checklist" includes:

- **Only needed rows are stored**: Regularly review what the application is storing to ensure no forgotten services are writing unused data.
- **Every column is used**: Engineers often lose track of columns, especially with ORM. Unfortunately, MySQL lacks a tool to find unused columns, so manual review comparing application queries to table schemas is necessary.
- **Every column is compact and practical**:
    - **Compact** means using the smallest data type possible (e.g., `SMALLINT` vs. `INT` if values are small).
    - **Practical** means the data type isn't so small it becomes difficult or error-prone to use.
    - The `VARCHAR(255)` antipattern is highlighted as an inefficient default; using `CHAR(2)` for atomic symbols, for instance, is more compact and practical.
    - ==**Avoid using `BLOB`, `TEXT`, and `JSON` data types as generic dumping grounds**; for images, better solutions like Amazon S3 exist.==
    - Using **unsigned integer data types** where values cannot be negative is more compact.
    - These are best practices in schema design and database performance, not micro-optimizations.
- **Every value is compact and practical**:
    - **Compact** means the smallest representation of the value, which depends on how the application uses it.
    - Values can be compacted by **minimizing**, **encoding**, and **deduplicating**.
        - **Minimizing** removes superfluous data like whitespace or comments from values (e.g., SQL statements). The most minimal value is `NULL`, which uses only one bit of storage; use `NULL` instead of empty strings, zeros, or magical values when practical, with `COALESCE()` for handling.
        - **Encoding** stores values in a more compact computer-encoded format (e.g., storing UUIDs as `BINARY(N)` and converting with `HEX()`/`UNHEX()` or `UUID_TO_BIN()`/`BIN_TO_TO_UUID()` in MySQL 8.0+). Dates and times should be stored and accessed as UTC.
        - **Deduplicating** values involves database normalization, separating data into related tables (e.g., a `genre` table for book genres, replacing long strings with small integer IDs). While not its primary goal, deduplication is a common and desired side effect of database normalization. Denormalization, the opposite, trades space for speed by intentionally duplicating data.
- **Every secondary index is used and not a duplicate**: Indexes are copies of data, so unused or duplicate secondary indexes increase data size.
    - An overabundance of indexes **increases index size** (using more RAM) and **decreases write performance** (as MySQL must update/reorganize every index on write).
    - Dropping unused/duplicate secondary indexes is an easy way to reduce data size, but should be done carefully to avoid causing full table scans.
    - Tools like `pt-duplicate-key-checker` can find duplicate indexes.
    - Index sizes can be checked using `INFORMATION_SCHEMA.TABLES` (for primary key `DATA_LENGTH` and secondary index `INDEX_LENGTH` per table/database) or `mysql.innodb_index_stats` (for individual index sizes).
- **Only needed rows are kept**: Data that is no longer needed should be deleted or archived. This completes the loop with "Only needed rows are stored".
