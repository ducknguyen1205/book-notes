
### 1. Indexes May Not Help

The first secret is that, ironically, **indexes may not help** improve performance, even though they are considered key to MySQL performance. The author notes that a majority of slow queries often already use an index lookup. This is counterintuitive because engineers usually focus on adding indexes to avoid table or index scans, leading to situations where only index lookups remain as an optimization target.

This secret is elaborated through two primary scenarios:

- **Finding Rows**:
    
    - Even with a good index, ==a query can examine too many rows, making the query slow.== The metric `rows examined` is crucial to check in such cases.
    - MySQL's `EXPLAIN` plan includes a `type` field that indicates the access type. Only specific access types (`system`, `const`, `eq_ref`, `unique_subquery`) are guaranteed to match at most one row. If the access type is not one of these, you should pay close attention to the `rows` field in the `EXPLAIN` plan and the `rows examined` query metric.
    - **Very low index selectivity** is often a culprit. If an index has very low selectivity (meaning it matches many rows), MySQL might choose to perform a full table scan instead of using that index, especially if there isn't a better index. To diagnose this, you can use `EXPLAIN` with `FORCE INDEX` to see the plan MySQL _isn't_ choosing.
    - Index statistics can sometimes be incorrect, leading MySQL to choose the wrong index. Running `ANALYZE TABLE` can update these statistics.
    - Index selectivity decreases if the number of rows increases while cardinality remains constant. An index that was effective on a small table might not be on a large one.
- **Joining Tables**:
    
    - When joining tables, even a few rows per table can "obliterate performance". This is due to the nested-loop join (NLJ) algorithm, where the total number of rows accessed for a join is the _product_ of the rows accessed for each table. For instance, a three-table join with 100 rows per table could access 1,000,000 rows (100 x 100 x 100).
    - To prevent this, the index lookup on each joined table should ideally match only one row, similar to the optimal single-row access types.
    - Additionally, indexes may not help if the **working set size** (frequently accessed data) becomes significantly larger than the available memory. In such cases, index usage can put pressure on storage I/O, slowing everything down. While more memory offers a quick fix, it's not a sustainable approach; the best solution involves addressing data size and access patterns, or sharding the database.

### 2. Less Data Is Better

This secret argues that **less data is better** for performance. While engineers often face the reality of growing databases, the author asserts that reducing data size improves performance because it requires fewer system resources like CPU, memory, and storage.

Key points supporting this secret include:

- Experienced engineers prefer and celebrate when data size is dramatically reduced because it improves performance, management, and cost efficiency. It's significantly faster, easier, and cheaper to handle smaller datasets (e.g., 100 GB vs. 100 TB).
- The primary problem is "unbridled data growth" rather than data size itself. It's common for engineers to hoard data, storing everything without considering if it's truly needed.
- The source advises monitoring data size and estimating future growth (e.g., a "four-year fit") to proactively address potential issues before they become critical. This is a key practice for anticipating when sharding might become necessary.

### 3. Less QPS Is Better

This is presented as a "counterintuitive" and "unpopular" secret. The author asserts that **less QPS (Queries Per Second) is better**, despite the common perception that higher QPS signifies better performance.

The reasoning behind this secret is based on three points:

- **QPS is only a number** â€“ a measurement of raw throughput. It provides no qualitative information about the queries or overall performance. An application might be idle at 10,000 QPS, while another is overloaded at 5,000 QPS, depending on the complexity and resource demands of the queries. High QPS is only valuable if query response time is also good.
- **QPS values have no objective meaning**. They are neither inherently good nor bad, high nor low; their meaning is always relative to a specific application or external events (e.g., time of day, holidays). A QPS of 100 could indicate an outage for one application but be normal for another.
- **It is difficult to increase QPS significantly**. Unlike data size, which can easily increase by 100x (e.g., 1 GB to 100 GB), achieving a 100x or even 2x increase in QPS can be very challenging. You cannot simply "purchase more QPS" like storage or memory.

In conclusion, the author states that QPS is more of a liability than an asset. Experienced engineers celebrate reducing QPS (intentionally) because it signifies increased capacity for growth.